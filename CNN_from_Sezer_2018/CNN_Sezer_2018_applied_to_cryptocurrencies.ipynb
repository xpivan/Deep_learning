{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from IPython.core.debugger import set_trace\n",
    "from glob import glob\n",
    "from talib.abstract import *\n",
    "import xarray as xr\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = 'YOUR_DATA_PATH'\n",
    "BINANCE_DATA = PATH_DATA+'BINANCE_PATH/*'\n",
    "list_all = glob(BINANCE_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak(func):\n",
    "    def algo(data, window):\n",
    "        max_local = func(data, window)\n",
    "        max_local = np.asarray(max_local)\n",
    "        max_local_valid = np.where(max_local==int(window/2))[0]\n",
    "        return max_local_valid\n",
    "    return algo\n",
    "    \n",
    "@get_peak\n",
    "def get_max_peak(data, window):\n",
    "    return data.rolling(window, center=True).apply(lambda x: np.where(x==np.max(x))[0][0], raw=True)\n",
    "\n",
    "@get_peak\n",
    "def get_min_peak(data, window):\n",
    "    return data.rolling(window, center=True).apply(lambda x: np.where(x==np.min(x))[0][0], raw=True)\n",
    "\n",
    "def return_min_max_peak(data, window=11):\n",
    "    i_x = get_max_peak(data['close'], window) #i_x stands for Index_maX\n",
    "    i_n = get_min_peak(data['close'], window) #i_n stands for Index_miN\n",
    "    return (i_x,i_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mm_scaler, vector):\n",
    "    norm_vector = mm_scaler.fit_transform(np.asarray(vector).reshape(-1,1))\n",
    "    norm_vector = np.reshape(norm_vector, len(norm_vector))\n",
    "    return norm_vector\n",
    "    \n",
    "def get_TA_4_model(daily, SMA_value, label):\n",
    "    mm_scaler = MinMaxScaler(feature_range=(0, 1)) \n",
    "    return \"FUNCTION NON AVAILABLE AND KEPT PRIVATE FOR COMPETITIVE REASON\"\n",
    "\n",
    "def remove_nan(data_tot, label_tot):\n",
    "    ind = np.argwhere(np.isnan(data_tot))\n",
    "    ind = np.unique(ind[:,0])\n",
    "    data_new = np.delete(data_tot, ind, axis=0)\n",
    "    label_new = np.delete(label_tot, ind)\n",
    "    return (data_new, label_new)\n",
    "\n",
    "def get_tot_size_time(list_file):\n",
    "    len_data = 0\n",
    "    for i, filename in enumerate(list_all):\n",
    "        data = pd.read_csv(filename)\n",
    "        len_data += len(data)\n",
    "    return len_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get total number of pictures we will produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pictures = get_tot_size_time(list_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peak detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = np.arange(6,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_time = datetime.datetime.now()\n",
    "data_tot = np.zeros((nb_pictures, 15, 15))\n",
    "label_tot = np.zeros(nb_pictures)\n",
    "ind_init = 0\n",
    "#foo = xr.DataArray(data, coords=[times, locs], dims=[\"time\", \"space\"])\n",
    "\n",
    "for i, filename in enumerate(list_all):\n",
    "    print('{}/{}'.format(i+1, len(list_all)))\n",
    "    data = pd.read_csv(filename)\n",
    "    label = np.zeros(len(data))\n",
    "    i_x, i_n = return_min_max_peak(data,window=11)\n",
    "    label[i_x]=1\n",
    "    label[i_n]=2\n",
    "    mat,label = get_TA_4_model(data, period, label)\n",
    "    lb = len(label)\n",
    "    data_tot[ind_init:ind_init+lb] = mat\n",
    "    label_tot[ind_init:ind_init+lb] = label\n",
    "    ind_init+=lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input, label = remove_nan(data_tot, label_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_input = np.shape(data_input)\n",
    "data_input = np.reshape(data_input, (shape_input[0], shape_input[1], shape_input[2], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_encoded = np.reshape(label,(len(label), 1))\n",
    "label_encoded = onehot_encoder.fit_transform(integer_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_cv, y_train, y_cv = train_test_split(data_input, label_encoded, train_size=0.8, test_size=0.2,\n",
    "                                                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv2D, Reshape, Flatten\n",
    "from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from tensorflow.keras import models, layers\n",
    "import tensorflow as tf\n",
    "from keras.utils.vis_utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Conv2D(32, (3,3), activation='relu', input_shape=(15,15,1), padding='same'))\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPooling2D(2,2))\n",
    "    model.add(layers.Dropout(rate=0.25))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dropout(rate=0.5))\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compile_model(model, METRICS, opt='adam', loss='categorical_crossentropy'):\n",
    "    \n",
    "    model.compile(optimizer=opt,\n",
    "              loss=loss,\n",
    "              metrics=METRICS)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def train_model(model, x_train, y_train, x_cv, y_cv, class_weight, EPOCHS):\n",
    "    \n",
    "    history = model.fit(x_train, y_train, epochs=EPOCHS, \n",
    "                    validation_data = (x_cv, y_cv),\n",
    "                    class_weight=class_weight)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model = compile_model(model, None, opt='adam', loss='categorical_crossentropy')\n",
    "\n",
    "plot_model(model, to_file='/Users/xavier/Projets/crypto/figures/Article/Linkedin/model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {\n",
    "    0:1,\n",
    "    1:3.2,\n",
    "    2:3.5\n",
    "}\n",
    "\n",
    "EPOCHS=5\n",
    "\n",
    "model = build_model()\n",
    "model = compile_model(model, None, opt='adam', loss='categorical_crossentropy')\n",
    "history = train_model(model, x_train, y_train, x_cv, y_cv, class_weight, EPOCHS=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.ylim([0.903, 0.916])\n",
    "plt.legend(loc='upper right')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
